{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DDPG: Deep Deterministic Policy Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial state:  [ 0.00505721  0.02772869  0.04208887 -0.04816065]\nsample action:  1\n#state:  4\n#action:  2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque \n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "state = env.reset()\n",
    "print(\"initial state: \", state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(\"sample action: \", action)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(\"#state: \", n_state)\n",
    "print(\"#action: \", n_action)"
   ]
  },
  {
   "source": [
    "## 2. Replay Memory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'terminal'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Experience(*args)) ## append a new experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): ## len(experience)\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "experience_pool = ReplayMemory(50000) #initialize memory pool"
   ]
  },
  {
   "source": [
    "## 3. Q-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "policy_net = nn.Sequential(\n",
    "        nn.Linear(n_state, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, n_action))\n",
    "\n",
    "target_net = nn.Sequential(\n",
    "        nn.Linear(n_state, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, n_action))\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict()) ## copy policy to target"
   ]
  },
  {
   "source": [
    "## 4. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1001\n",
    "EPOCH_STEPS = 200\n",
    "BATCH_SIZE = 32 #batch-train\n",
    "WARM_UP_SIZE = BATCH_SIZE\n",
    "GAMMA = 0.99 #reward-discount: 0.99 is better than 0.90 !!!!!!\n",
    "EPS_GREEDY = 0.1 #epsilon-greedy\n",
    "EPS_DEC = 1e-6 #adaptive epsilon greedy\n",
    "TARGET_UPDATE = 200 #policy to target\n",
    "\n",
    "#optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "source": [
    "## 5. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_net():\n",
    "    experiences = experience_pool.sample(BATCH_SIZE)\n",
    "    experiences_batch = Experience(*zip(*experiences)) #experiences of batches\n",
    "\n",
    "    state_batch = torch.stack(experiences_batch.state)\n",
    "    action_batch = torch.stack(experiences_batch.action)\n",
    "    reward_batch = torch.stack(experiences_batch.reward)\n",
    "    next_state_batch = torch.stack(experiences_batch.next_state)\n",
    "    terminal_batch = torch.stack(experiences_batch.terminal)\n",
    "\n",
    "    output_policy = policy_net(state_batch)\n",
    "    policy_q_value = torch.squeeze(torch.gather(output_policy, 1, action_batch))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_target_next = target_net(next_state_batch)\n",
    "        target_next_q_value = torch.max(output_target_next, dim=1).values\n",
    "\n",
    "    target_q_value = reward_batch + GAMMA * target_next_q_value * (1 - terminal_batch)\n",
    "\n",
    "    # mean square loss\n",
    "    loss = loss_fn(policy_q_value, target_q_value)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "## 6. Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def greedy_action(state): # state is tensor\n",
    "    with torch.no_grad():\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "    return action #return integer\n",
    "\n",
    "def sample_action(state):\n",
    "    global EPS_GREEDY\n",
    "    current_eps = np.maximum(0.01, EPS_GREEDY)\n",
    "    EPS_GREEDY -= EPS_DEC\n",
    "    if np.random.rand() < (1.0 - current_eps): #choose a by policy-NN\n",
    "        action = greedy_action(state) #greedy\n",
    "    else:\n",
    "        action = np.random.randint(n_action) #random action\n",
    "\n",
    "    return torch.tensor([action], dtype=torch.int64) #return tensor\n",
    "\n",
    "def explore_one_step(state, pool):\n",
    "    action = sample_action(state) # a\n",
    "    obs, r, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor(r, dtype=torch.float) # r\n",
    "    next_state = torch.tensor(obs, dtype=torch.float) # s'\n",
    "    terminal = torch.tensor(int(done) * 1.0, dtype=torch.float) # t\n",
    "\n",
    "    # Store the transition in experience pool\n",
    "    pool.push(state, action, reward, next_state, terminal) #(s,a,r,s',t), tensors\n",
    "\n",
    "    return done, next_state, r"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float)\n",
    "    env.render()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float)\n",
    "        if done:\n",
    "            break # one episode\n",
    "\n",
    "def train_loop():\n",
    "    update_policy_steps = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        explore_steps = 0\n",
    "        reward = 0\n",
    "        # Initialize the environment and state\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float) # s\n",
    "        while explore_steps < EPOCH_STEPS:\n",
    "            explore_steps += 1\n",
    "            # generate experience\n",
    "            done, next_state, r = explore_one_step(state, experience_pool)\n",
    "            state = next_state\n",
    "            reward += r\n",
    "            # Perform one step of the optimization\n",
    "            if len(experience_pool) > WARM_UP_SIZE:\n",
    "                update_policy_net()\n",
    "                update_policy_steps += 1\n",
    "                # Update the target network, copying all weights and biases from policy network\n",
    "                if update_policy_steps % TARGET_UPDATE == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "            if done:\n",
    "                break # one episode\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch: \", epoch, \"reward: \", reward)\n",
    "            evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop()"
   ]
  },
  {
   "source": [
    "## 8. Load Saved Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy_net.state_dict(), 'policy-1.pt')\n",
    "policy_net.load_state_dict(torch.load('policy-1.pt'))\n",
    "\n",
    "evaluate()"
   ]
  }
 ]
}