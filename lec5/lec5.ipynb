{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DDPG: Deep Deterministic Policy Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial state:  [ 0.04033672  0.01563276 -0.00796419 -0.03678735]\nsample action:  [-0.32881314]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque \n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import cartenv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = cartenv.ContinuousCartPoleEnv()\n",
    "\n",
    "state = env.reset()\n",
    "print(\"initial state: \", state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(\"sample action: \", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Box(1,)\nBox(4,)\n#state:  4\n#action:  1\n[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n[1.]\n[-1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "n_action = env.action_space.shape[0]\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(\"#state: \", n_state)\n",
    "print(\"#action: \", n_action)\n",
    "\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.low)"
   ]
  },
  {
   "source": [
    "## 2. Experience Pool"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'terminal'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Experience(*args)) ## append a new experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): ## len(experience)\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "experience_pool = ReplayMemory(int(1e6)) #initialize memory pool"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "## 3. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6001\n",
    "EPOCH_STEPS = 200\n",
    "BATCH_SIZE = 128 #batch-train\n",
    "WARM_UP_SIZE = BATCH_SIZE\n",
    "GAMMA = 0.999 #reward-discount: 0.99 vs 0.999???\n",
    "EXPLORE_NOISE = 0.05 #the best choice?\n",
    "UPDATE_WEIGHT = 0.999 #0.99 vs 0.999???\n",
    "LEARN_RATE = 1e-3 "
   ]
  },
  {
   "source": [
    "## 4. Policy-Network & Q-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = nn.Sequential(\n",
    "        nn.Linear(n_state, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, n_action),\n",
    "        nn.Tanh()) #tanh\n",
    "\n",
    "q_net = nn.Sequential(\n",
    "        nn.Linear(n_state + n_action, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 1))\n",
    "\n",
    "target_p_net = deepcopy(policy_net)\n",
    "target_q_net = deepcopy(q_net)\n",
    "\n",
    "def enable_gradient(network):\n",
    "        for p in network.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "def disable_gradient(network):\n",
    "        for p in network.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "disable_gradient(target_p_net)\n",
    "disable_gradient(target_q_net)\n",
    "\n",
    "def copy_net(source_net, target_net):\n",
    "        with torch.no_grad():\n",
    "                for p, p_targ in zip(source_net.parameters(), target_net.parameters()):\n",
    "                        p_targ.data.mul_(UPDATE_WEIGHT)\n",
    "                        p_targ.data.add_((1 - UPDATE_WEIGHT) * p.data)"
   ]
  },
  {
   "source": [
    "## 5. Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_action(state): # state is tensor\n",
    "    with torch.no_grad():\n",
    "        action = env.action_space.high[0] * policy_net(state)\n",
    "    return action\n",
    "\n",
    "def explore_action(state):\n",
    "    with torch.no_grad():\n",
    "        action = env.action_space.high[0] * policy_net(state)\n",
    "        action = torch.normal(action, EXPLORE_NOISE)\n",
    "        action = torch.clamp(action, min=env.action_space.low[0], max=env.action_space.high[0])\n",
    "    return action\n",
    "\n",
    "def target_action(state):\n",
    "    return env.action_space.high[0] * target_p_net(state)\n",
    "\n",
    "def explore_one_step(state):\n",
    "    action = explore_action(state) # a\n",
    "    obs, r, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor(r, dtype=torch.float) # r\n",
    "    next_state = torch.tensor(obs, dtype=torch.float) # s'\n",
    "    terminal = torch.tensor(int(done) * 1.0, dtype=torch.float) # t\n",
    "\n",
    "    # Store the transition in experience pool\n",
    "    experience_pool.push(state, action, reward, next_state, terminal) #(s,a,r,s',t), tensors\n",
    "\n",
    "    return done, next_state, r"
   ]
  },
  {
   "source": [
    "## 6. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "optimizer_p = optim.SGD(policy_net.parameters(), lr=LEARN_RATE)\n",
    "optimizer_q = optim.SGD(q_net.parameters(), lr=LEARN_RATE)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def sample_batch():\n",
    "    experiences = experience_pool.sample(BATCH_SIZE)\n",
    "    experiences_batch = Experience(*zip(*experiences)) #experiences of batches, unpack twice\n",
    "\n",
    "    state_batch = torch.stack(experiences_batch.state)\n",
    "    action_batch = torch.stack(experiences_batch.action)\n",
    "    reward_batch = torch.stack(experiences_batch.reward)\n",
    "    next_state_batch = torch.stack(experiences_batch.next_state)\n",
    "    terminal_batch = torch.stack(experiences_batch.terminal)\n",
    "    state_action_batch = torch.cat((state_batch, action_batch), dim=1)\n",
    "    return state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, state_action_batch\n",
    "\n",
    "def update_q_net(r, ns, d, sa):\n",
    "    curr_q_value = q_net(sa).squeeze()\n",
    "\n",
    "    next_action = target_p_net(ns)\n",
    "    next_sa = torch.cat((ns, next_action), dim=1)\n",
    "    target_next_q_value = target_q_net(next_sa).squeeze()\n",
    "\n",
    "    target_q_value = r + GAMMA * target_next_q_value * (1 - d)\n",
    "\n",
    "    # mean square loss\n",
    "    loss = loss_fn(curr_q_value, target_q_value)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer_q.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_q.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def update_policy_net(s):\n",
    "    curr_action = policy_net(s)\n",
    "    curr_sa = torch.cat((s, curr_action), dim=1)\n",
    "\n",
    "    ## using q network\n",
    "    disable_gradient(q_net)\n",
    "    loss = -1.0 * torch.mean(q_net(curr_sa))\n",
    "    # Optimize the model\n",
    "    optimizer_p.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_p.step()\n",
    "    enable_gradient(q_net)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float)\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = policy_action(state).item()\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float)\n",
    "        if done:\n",
    "            break # one episode\n",
    "\n",
    "def train_loop():\n",
    "    for epoch in range(EPOCHS):\n",
    "        explore_steps = 0\n",
    "        reward = 0\n",
    "        # Initialize the environment and state\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float) # s\n",
    "        while explore_steps < EPOCH_STEPS:\n",
    "            explore_steps += 1\n",
    "            # generate experience\n",
    "            done, next_state, r = explore_one_step(state)\n",
    "            state = next_state\n",
    "            reward += r\n",
    "            # Perform one step of the optimization\n",
    "            if len(experience_pool) > WARM_UP_SIZE:\n",
    "                s, _, r, ns, d, sa = sample_batch()\n",
    "                loss_q = update_q_net(r,ns,d,sa)\n",
    "                loss_p = update_policy_net(s)\n",
    "                copy_net(policy_net, target_p_net)\n",
    "                copy_net(q_net, target_q_net)\n",
    "\n",
    "            if done:\n",
    "                break # one episode\n",
    "\n",
    "        if epoch % 50 == 0 and len(experience_pool) > WARM_UP_SIZE:\n",
    "            evaluate()\n",
    "            print(\"epoch: \", epoch, \"reward: \", reward, \"loss_policy: \", loss_p, \"loss_q: \", loss_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop()"
   ]
  },
  {
   "source": [
    "## 8. Load Saved Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy_net.state_dict(), 'policy.pt')\n",
    "#policy_net.load_state_dict(torch.load('policy.pt'))\n",
    "\n",
    "#evaluate()"
   ]
  }
 ]
}