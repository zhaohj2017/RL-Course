{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DDPG: Deep Deterministic Policy Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial state:  [ 0.03352889  0.0183511  -0.04621113 -0.00925396]\nsample action:  [0.68621486]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque \n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import cartenv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = cartenv.ContinuousCartPoleEnv()\n",
    "\n",
    "state = env.reset()\n",
    "print(\"initial state: \", state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(\"sample action: \", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Box(1,)\nBox(4,)\n#state:  4\n#action:  1\n[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n[1.]\n[-1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "n_action = env.action_space.shape[0]\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(\"#state: \", n_state)\n",
    "print(\"#action: \", n_action)\n",
    "\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.low)"
   ]
  },
  {
   "source": [
    "## 2. Experience Pool"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'terminal'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Experience(*args)) ## append a new experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): ## len(experience)\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "experience_pool = ReplayMemory(10000000) #initialize memory pool"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "## 3. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6001\n",
    "EPOCH_STEPS = 200\n",
    "BATCH_SIZE = 128 #batch-train\n",
    "WARM_UP_SIZE = BATCH_SIZE\n",
    "GAMMA = 0.99 #reward-discount\n",
    "EXPLORE_NOISE = 0.05\n",
    "UPDATE_WEIGHT = 0.999\n",
    "LEARN_RATE = 1e-3 "
   ]
  },
  {
   "source": [
    "## 4. Policy-Network & Q-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = nn.Sequential(\n",
    "        nn.Linear(n_state, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, n_action),\n",
    "        nn.Tanh()) #tanh\n",
    "\n",
    "q_net = nn.Sequential(\n",
    "        nn.Linear(n_state + n_action, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 1))\n",
    "\n",
    "target_p_net = deepcopy(policy_net)\n",
    "target_q_net = deepcopy(q_net)\n",
    "\n",
    "def enable_gradient(network):\n",
    "        for p in network.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "def disable_gradient(network):\n",
    "        for p in network.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "disable_gradient(target_p_net)\n",
    "disable_gradient(target_q_net)\n",
    "\n",
    "def copy_net(source_net, target_net):\n",
    "        with torch.no_grad():\n",
    "                for p, p_targ in zip(source_net.parameters(), target_net.parameters()):\n",
    "                        p_targ.data.mul_(UPDATE_WEIGHT)\n",
    "                        p_targ.data.add_((1 - UPDATE_WEIGHT) * p.data)"
   ]
  },
  {
   "source": [
    "## 5. Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_action(state): # state is tensor\n",
    "    with torch.no_grad():\n",
    "        action = env.action_space.high[0] * policy_net(state)\n",
    "    return action\n",
    "\n",
    "def explore_action(state):\n",
    "    with torch.no_grad():\n",
    "        action = env.action_space.high[0] * policy_net(state)\n",
    "        action = torch.normal(action, EXPLORE_NOISE)\n",
    "        action = torch.clamp(action, min=env.action_space.low[0], max=env.action_space.high[0])\n",
    "    return action\n",
    "\n",
    "def target_action(state):\n",
    "    return env.action_space.high[0] * target_p_net(state)\n",
    "\n",
    "def explore_one_step(state):\n",
    "    action = explore_action(state) # a\n",
    "    obs, r, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor(r, dtype=torch.float) # r\n",
    "    next_state = torch.tensor(obs, dtype=torch.float) # s'\n",
    "    terminal = torch.tensor(int(done) * 1.0, dtype=torch.float) # t\n",
    "\n",
    "    # Store the transition in experience pool\n",
    "    experience_pool.push(state, action, reward, next_state, terminal) #(s,a,r,s',t), tensors\n",
    "\n",
    "    return done, next_state, r"
   ]
  },
  {
   "source": [
    "## 6. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "optimizer_p = optim.SGD(policy_net.parameters(), lr=LEARN_RATE)\n",
    "optimizer_q = optim.SGD(q_net.parameters(), lr=LEARN_RATE)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def sample_batch():\n",
    "    experiences = experience_pool.sample(BATCH_SIZE)\n",
    "    experiences_batch = Experience(*zip(*experiences)) #experiences of batches, unpack twice\n",
    "\n",
    "    state_batch = torch.stack(experiences_batch.state)\n",
    "    action_batch = torch.stack(experiences_batch.action)\n",
    "    reward_batch = torch.stack(experiences_batch.reward)\n",
    "    next_state_batch = torch.stack(experiences_batch.next_state)\n",
    "    terminal_batch = torch.stack(experiences_batch.terminal)\n",
    "    state_action_batch = torch.cat((state_batch, action_batch), dim=1)\n",
    "    return state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, state_action_batch\n",
    "\n",
    "def update_q_net(r, ns, d, sa):\n",
    "    curr_q_value = q_net(sa).squeeze()\n",
    "\n",
    "    next_action = target_p_net(ns)\n",
    "    next_sa = torch.cat((ns, next_action), dim=1)\n",
    "    target_next_q_value = target_q_net(next_sa).squeeze()\n",
    "\n",
    "    target_q_value = r + GAMMA * target_next_q_value * (1 - d)\n",
    "\n",
    "    # mean square loss\n",
    "    loss = loss_fn(curr_q_value, target_q_value)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer_q.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_q.step()\n",
    "\n",
    "def update_policy_net(s):\n",
    "    curr_action = policy_net(s)\n",
    "    curr_sa = torch.cat((s, curr_action), dim=1)\n",
    "\n",
    "    ## using q network\n",
    "    disable_gradient(q_net)\n",
    "    loss = -1.0 * torch.mean(q_net(curr_sa))\n",
    "    # Optimize the model\n",
    "    optimizer_p.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_p.step()\n",
    "    enable_gradient(q_net)"
   ]
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float)\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = policy_action(state).item()\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float)\n",
    "        if done:\n",
    "            break # one episode\n",
    "\n",
    "def train_loop():\n",
    "    for epoch in range(EPOCHS):\n",
    "        explore_steps = 0\n",
    "        reward = 0\n",
    "        # Initialize the environment and state\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float) # s\n",
    "        while explore_steps < EPOCH_STEPS:\n",
    "            explore_steps += 1\n",
    "            # generate experience\n",
    "            done, next_state, r = explore_one_step(state)\n",
    "            state = next_state\n",
    "            reward += r\n",
    "            # Perform one step of the optimization\n",
    "            if len(experience_pool) > WARM_UP_SIZE:\n",
    "                s, _, r, ns, d, sa = sample_batch()\n",
    "                update_q_net(r,ns,d,sa)\n",
    "                update_policy_net(s)\n",
    "                copy_net(policy_net, target_p_net)\n",
    "                copy_net(q_net, target_q_net)\n",
    "\n",
    "            if done:\n",
    "                break # one episode\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            evaluate()\n",
    "            print(\"epoch: \", epoch, \"reward: \", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  0 reward:  12.0\n",
      "epoch:  50 reward:  16.0\n",
      "epoch:  100 reward:  8.0\n",
      "epoch:  150 reward:  7.0\n",
      "epoch:  200 reward:  7.0\n",
      "epoch:  250 reward:  7.0\n",
      "epoch:  300 reward:  6.0\n",
      "epoch:  350 reward:  7.0\n",
      "epoch:  400 reward:  6.0\n",
      "epoch:  450 reward:  6.0\n",
      "epoch:  500 reward:  6.0\n",
      "epoch:  550 reward:  7.0\n",
      "epoch:  600 reward:  6.0\n",
      "epoch:  650 reward:  6.0\n",
      "epoch:  700 reward:  6.0\n",
      "epoch:  750 reward:  6.0\n",
      "epoch:  800 reward:  6.0\n",
      "epoch:  850 reward:  6.0\n",
      "epoch:  900 reward:  6.0\n",
      "epoch:  950 reward:  6.0\n",
      "epoch:  1000 reward:  6.0\n",
      "epoch:  1050 reward:  6.0\n",
      "epoch:  1100 reward:  6.0\n",
      "epoch:  1150 reward:  18.0\n",
      "epoch:  1200 reward:  27.0\n",
      "epoch:  1250 reward:  44.0\n",
      "epoch:  1300 reward:  65.0\n",
      "epoch:  1350 reward:  41.0\n",
      "epoch:  1400 reward:  76.0\n",
      "epoch:  1450 reward:  74.0\n",
      "epoch:  1500 reward:  166.0\n",
      "epoch:  1550 reward:  200.0\n",
      "epoch:  1600 reward:  200.0\n",
      "epoch:  1650 reward:  200.0\n",
      "epoch:  1700 reward:  200.0\n",
      "epoch:  1750 reward:  200.0\n",
      "epoch:  1800 reward:  200.0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6590b82f66d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-dce2ca086a10>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reward: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dce2ca086a10>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/2020秋/papers to read/reinforcement-learning/videos/baidu/RL-Course/lec5/cartenv.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;34m\"%r (%s) invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Cast action to float to strip np trappings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gym/spaces/box.py\u001b[0m in \u001b[0;36mcontains\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Promote list to array for contains check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_jsonable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mall\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \"\"\"\n\u001b[0;32m-> 2398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "source": [
    "## 8. Load Saved Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy_net.state_dict(), 'policy.pt')\n",
    "#policy_net.load_state_dict(torch.load('policy.pt'))\n",
    "\n",
    "#evaluate()"
   ]
  }
 ]
}