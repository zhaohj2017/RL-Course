{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DDPG: Deep Deterministic Policy Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial state:  [ 0.03347734 -0.03412536  0.04532031 -0.04244521]\nsample action:  [0.7076171]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque \n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import cartenv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = cartenv.ContinuousCartPoleEnv()\n",
    "\n",
    "state = env.reset()\n",
    "print(\"initial state: \", state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(\"sample action: \", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Box(1,)\nBox(4,)\n#state:  4\n#action:  1\n[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n[1.]\n[-1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "n_action = env.action_space.shape[0]\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(\"#state: \", n_state)\n",
    "print(\"#action: \", n_action)\n",
    "\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.low)"
   ]
  },
  {
   "source": [
    "## 2. Experience Pool"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'terminal'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Experience(*args)) ## append a new experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): ## len(experience)\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "experience_pool = ReplayMemory(int(1e6)) #initialize memory pool"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "## 3. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6001\n",
    "EPOCH_STEPS = 200\n",
    "BATCH_SIZE = 128 #batch-train\n",
    "WARM_UP_SIZE = BATCH_SIZE\n",
    "GAMMA = 0.99 #reward-discount: 0.99 vs 0.999???\n",
    "EXPLORE_NOISE = 0.05 #the best choice?\n",
    "UPDATE_WEIGHT = 0.999 #0.99 vs 0.999???\n",
    "LEARN_RATE = 1e-3 "
   ]
  },
  {
   "source": [
    "## 4. Policy-Network & Q-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = nn.Sequential(\n",
    "        nn.Linear(n_state, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, n_action),\n",
    "        nn.Tanh()) #tanh\n",
    "\n",
    "q_net = nn.Sequential(\n",
    "        nn.Linear(n_state + n_action, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 1))\n",
    "\n",
    "target_p_net = deepcopy(policy_net)\n",
    "target_q_net = deepcopy(q_net)\n",
    "\n",
    "def enable_gradient(network):\n",
    "        for p in network.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "def disable_gradient(network):\n",
    "        for p in network.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "disable_gradient(target_p_net)\n",
    "disable_gradient(target_q_net)\n",
    "\n",
    "def copy_net(source_net, target_net):\n",
    "        with torch.no_grad():\n",
    "                for p, p_targ in zip(source_net.parameters(), target_net.parameters()):\n",
    "                        p_targ.data.mul_(UPDATE_WEIGHT)\n",
    "                        p_targ.data.add_((1 - UPDATE_WEIGHT) * p.data)"
   ]
  },
  {
   "source": [
    "## 5. Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_action(state): # state is tensor\n",
    "    with torch.no_grad():\n",
    "        action = env.action_space.high[0] * policy_net(state)\n",
    "    return action\n",
    "\n",
    "def explore_action(state):\n",
    "    with torch.no_grad():\n",
    "        action = env.action_space.high[0] * policy_net(state)\n",
    "        action = torch.normal(action, EXPLORE_NOISE)\n",
    "        action = torch.clamp(action, min=env.action_space.low[0], max=env.action_space.high[0])\n",
    "    return action\n",
    "\n",
    "def target_action(state):\n",
    "    return env.action_space.high[0] * target_p_net(state)\n",
    "\n",
    "def explore_one_step(state):\n",
    "    action = explore_action(state) # a\n",
    "    obs, r, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor(r, dtype=torch.float) # r\n",
    "    next_state = torch.tensor(obs, dtype=torch.float) # s'\n",
    "    terminal = torch.tensor(int(done) * 1.0, dtype=torch.float) # t\n",
    "\n",
    "    # Store the transition in experience pool\n",
    "    experience_pool.push(state, action, reward, next_state, terminal) #(s,a,r,s',t), tensors\n",
    "\n",
    "    return done, next_state, r"
   ]
  },
  {
   "source": [
    "## 6. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_p = optim.SGD(policy_net.parameters(), lr=LEARN_RATE)\n",
    "# optimizer_q = optim.SGD(q_net.parameters(), lr=LEARN_RATE)\n",
    "optimizer_p = optim.Adam(policy_net.parameters(), lr=LEARN_RATE)\n",
    "optimizer_q = optim.Adam(q_net.parameters(), lr=LEARN_RATE)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def sample_batch():\n",
    "    experiences = experience_pool.sample(BATCH_SIZE)\n",
    "    experiences_batch = Experience(*zip(*experiences)) #experiences of batches, unpack twice\n",
    "\n",
    "    state_batch = torch.stack(experiences_batch.state)\n",
    "    action_batch = torch.stack(experiences_batch.action)\n",
    "    reward_batch = torch.stack(experiences_batch.reward)\n",
    "    next_state_batch = torch.stack(experiences_batch.next_state)\n",
    "    terminal_batch = torch.stack(experiences_batch.terminal)\n",
    "    state_action_batch = torch.cat((state_batch, action_batch), dim=1)\n",
    "    return state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, state_action_batch\n",
    "\n",
    "def update_q_net(r, ns, d, sa):\n",
    "    curr_q_value = q_net(sa).squeeze()\n",
    "\n",
    "    next_action = target_p_net(ns)\n",
    "    next_sa = torch.cat((ns, next_action), dim=1)\n",
    "    target_next_q_value = target_q_net(next_sa).squeeze()\n",
    "\n",
    "    target_q_value = r + GAMMA * target_next_q_value * (1 - d)\n",
    "\n",
    "    # mean square loss\n",
    "    loss = loss_fn(curr_q_value, target_q_value)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer_q.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_q.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def update_policy_net(s):\n",
    "    curr_action = policy_net(s)\n",
    "    curr_sa = torch.cat((s, curr_action), dim=1)\n",
    "\n",
    "    ## using q network\n",
    "    disable_gradient(q_net)\n",
    "    loss = -1.0 * torch.mean(q_net(curr_sa))\n",
    "    # Optimize the model\n",
    "    optimizer_p.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_p.step()\n",
    "    enable_gradient(q_net)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float)\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = policy_action(state).item()\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float)\n",
    "        if done:\n",
    "            break # one episode\n",
    "\n",
    "def train_loop():\n",
    "    for epoch in range(EPOCHS):\n",
    "        explore_steps = 0\n",
    "        reward = 0\n",
    "        # Initialize the environment and state\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float) # s\n",
    "        while explore_steps < EPOCH_STEPS:\n",
    "            explore_steps += 1\n",
    "            # generate experience\n",
    "            done, next_state, r = explore_one_step(state)\n",
    "            state = next_state\n",
    "            reward += r\n",
    "            # Perform one step of the optimization\n",
    "            if len(experience_pool) > WARM_UP_SIZE:\n",
    "                s, _, r, ns, d, sa = sample_batch()\n",
    "                loss_q = update_q_net(r,ns,d,sa)\n",
    "                loss_p = update_policy_net(s)\n",
    "                copy_net(policy_net, target_p_net)\n",
    "                copy_net(q_net, target_q_net)\n",
    "\n",
    "            if done:\n",
    "                break # one episode\n",
    "\n",
    "        if epoch % 50 == 0 and len(experience_pool) > WARM_UP_SIZE:\n",
    "            evaluate()\n",
    "            print(\"epoch: \", epoch, \"reward: \", reward, \"loss_policy: \", loss_p, \"loss_q: \", loss_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\tsteps:  1\n",
      "\tsteps:  2\n",
      "\tsteps:  3\n",
      "\tsteps:  4\n",
      "\tsteps:  5\n",
      "\tsteps:  6\n",
      "epoch:  50 reward:  6.0 loss_policy:  -1.2291204929351807 loss_q:  0.004729376174509525\n",
      "\tsteps:  1\n",
      "\tsteps:  2\n",
      "\tsteps:  3\n",
      "\tsteps:  4\n",
      "\tsteps:  5\n",
      "\tsteps:  6\n",
      "epoch:  100 reward:  6.0 loss_policy:  -1.444199562072754 loss_q:  0.009121711365878582\n",
      "\tsteps:  1\n",
      "\tsteps:  2\n",
      "\tsteps:  3\n",
      "\tsteps:  4\n",
      "\tsteps:  5\n",
      "epoch:  150 reward:  6.0 loss_policy:  -1.6171875 loss_q:  0.0135640325024724\n",
      "\tsteps:  1\n",
      "\tsteps:  2\n",
      "\tsteps:  3\n",
      "\tsteps:  4\n",
      "\tsteps:  5\n",
      "\tsteps:  6\n",
      "epoch:  200 reward:  7.0 loss_policy:  -1.823456883430481 loss_q:  0.008303007110953331\n",
      "\tsteps:  1\n",
      "\tsteps:  2\n",
      "\tsteps:  3\n",
      "\tsteps:  4\n",
      "\tsteps:  5\n",
      "\tsteps:  6\n",
      "\tsteps:  7\n",
      "\tsteps:  8\n",
      "\tsteps:  9\n",
      "\tsteps:  10\n",
      "\tsteps:  11\n",
      "\tsteps:  12\n",
      "\tsteps:  13\n",
      "\tsteps:  14\n",
      "\tsteps:  15\n",
      "\tsteps:  16\n",
      "epoch:  250 reward:  15.0 loss_policy:  -2.197082996368408 loss_q:  0.009018969722092152\n",
      "\tsteps:  1\n",
      "\tsteps:  2\n",
      "\tsteps:  3\n",
      "\tsteps:  4\n",
      "\tsteps:  5\n",
      "\tsteps:  6\n",
      "\tsteps:  7\n",
      "\tsteps:  8\n",
      "\tsteps:  9\n",
      "epoch:  300 reward:  13.0 loss_policy:  -2.6584808826446533 loss_q:  0.025904055684804916\n",
      "\tsteps:  1\n",
      "\tsteps:  2\n",
      "\tsteps:  3\n",
      "\tsteps:  4\n",
      "\tsteps:  5\n",
      "\tsteps:  6\n",
      "\tsteps:  7\n",
      "\tsteps:  8\n",
      "\tsteps:  9\n",
      "epoch:  350 reward:  10.0 loss_policy:  -3.0057191848754883 loss_q:  0.017221413552761078\n",
      "\tsteps:  1\n",
      "\tsteps:  2\n",
      "\tsteps:  3\n",
      "\tsteps:  4\n",
      "\tsteps:  5\n",
      "\tsteps:  6\n",
      "\tsteps:  7\n",
      "\tsteps:  8\n",
      "\tsteps:  9\n",
      "\tsteps:  10\n",
      "\tsteps:  11\n",
      "\tsteps:  12\n",
      "\tsteps:  13\n",
      "\tsteps:  14\n",
      "\tsteps:  15\n",
      "\tsteps:  16\n",
      "\tsteps:  17\n",
      "\tsteps:  18\n",
      "\tsteps:  19\n",
      "\tsteps:  20\n",
      "\tsteps:  21\n",
      "\tsteps:  22\n",
      "\tsteps:  23\n",
      "\tsteps:  24\n",
      "epoch:  400 reward:  15.0 loss_policy:  -3.371758222579956 loss_q:  0.015535427257418633\n",
      "\tsteps:  1\n",
      "\tsteps:  2\n",
      "\tsteps:  3\n",
      "\tsteps:  4\n",
      "\tsteps:  5\n",
      "\tsteps:  6\n",
      "\tsteps:  7\n",
      "\tsteps:  8\n",
      "\tsteps:  9\n",
      "\tsteps:  10\n",
      "\tsteps:  11\n",
      "\tsteps:  12\n",
      "\tsteps:  13\n",
      "\tsteps:  14\n",
      "\tsteps:  15\n",
      "\tsteps:  16\n",
      "\tsteps:  17\n",
      "\tsteps:  18\n",
      "epoch:  450 reward:  52.0 loss_policy:  -4.27121639251709 loss_q:  0.03928437456488609\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6590b82f66d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-14482ad32d5b>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mloss_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_q_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mloss_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mcopy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_p_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mcopy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e7a93a5f0578>\u001b[0m in \u001b[0;36mupdate_policy_net\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_sa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0moptimizer_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0moptimizer_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "source": [
    "## 8. Load Saved Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy_net.state_dict(), 'policy.pt')\n",
    "#policy_net.load_state_dict(torch.load('policy.pt'))\n",
    "\n",
    "#evaluate()"
   ]
  }
 ]
}