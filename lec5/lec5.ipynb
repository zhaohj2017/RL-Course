{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DDPG: Deep Deterministic Policy Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial state:  [-0.0158464   0.00129966  0.04116796  0.03368109]\nsample action:  [-0.90528506]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque \n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import cartenv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = cartenv.ContinuousCartPoleEnv()\n",
    "\n",
    "state = env.reset()\n",
    "print(\"initial state: \", state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(\"sample action: \", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Box(1,)\nBox(4,)\n#state:  4\n#action:  1\n[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n[1.]\n[-1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "n_action = env.action_space.shape[0]\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(\"#state: \", n_state)\n",
    "print(\"#action: \", n_action)\n",
    "\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.low)"
   ]
  },
  {
   "source": [
    "## 2. Experience Pool"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'terminal'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Experience(*args)) ## append a new experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): ## len(experience)\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "experience_pool = ReplayMemory(50000) #initialize memory pool"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 13,
   "outputs": []
  },
  {
   "source": [
    "## 3. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1001\n",
    "EPOCH_STEPS = 200\n",
    "BATCH_SIZE = 32 #batch-train\n",
    "WARM_UP_SIZE = BATCH_SIZE\n",
    "GAMMA = 0.99 #reward-discount: 0.99 is better than 0.90 !!!!!!\n",
    "EPS_GREEDY = 0.1 #epsilon-greedy\n",
    "EPS_DEC = 1e-6 #adaptive epsilon greedy\n",
    "TARGET_UPDATE = 200 #policy to target\n",
    "EXPLORE_NOISE = 0.05\n",
    "UPDATE_WEIGHT = 0.99"
   ]
  },
  {
   "source": [
    "## 4. Policy-Network & Q-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = nn.Sequential(\n",
    "        nn.Linear(n_state, 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, n_action),\n",
    "        nn.Tanh()) #tanh\n",
    "\n",
    "q_net = nn.Sequential(\n",
    "        nn.Linear(n_state + n_action, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 1))\n",
    "\n",
    "target_p_net = deepcopy(policy_net)\n",
    "target_q_net = deepcopy(q_net)\n",
    "\n",
    "def disable_gradient(network):\n",
    "        for p in network.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "disable_gradient(target_p_net)\n",
    "disable_gradient(target_q_net)\n",
    "\n",
    "def copy_net(source_net, target_net):\n",
    "        with torch.no_grad():\n",
    "                for p, p_targ in zip(source_net.parameters(), target_net.parameters()):\n",
    "                        p_targ.data.mul_(UPDATE_WEIGHT)\n",
    "                        p_targ.data.add_((1 - UPDATE_WEIGHT) * p.data)"
   ]
  },
  {
   "source": [
    "## 5. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def update_policy_net():\n",
    "    experiences = experience_pool.sample(BATCH_SIZE)\n",
    "    experiences_batch = Experience(*zip(*experiences)) #experiences of batches\n",
    "\n",
    "    state_batch = torch.stack(experiences_batch.state)\n",
    "    action_batch = torch.stack(experiences_batch.action)\n",
    "    reward_batch = torch.stack(experiences_batch.reward)\n",
    "    next_state_batch = torch.stack(experiences_batch.next_state)\n",
    "    terminal_batch = torch.stack(experiences_batch.terminal)\n",
    "\n",
    "    output_policy = policy_net(state_batch)\n",
    "    policy_q_value = torch.squeeze(torch.gather(output_policy, 1, action_batch))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_target_next = target_net(next_state_batch)\n",
    "        target_next_q_value = torch.max(output_target_next, dim=1).values\n",
    "\n",
    "    target_q_value = reward_batch + GAMMA * target_next_q_value * (1 - terminal_batch)\n",
    "\n",
    "    # mean square loss\n",
    "    loss = loss_fn(policy_q_value, target_q_value)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "## 6. Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def policy_action(state): # state is tensor\n",
    "    return policy_net(state)\n",
    "\n",
    "def explore_action(state):\n",
    "    return torch.normal(policy_net(state), NOISE)\n",
    "\n",
    "def target_action():\n",
    "    return target_p_net(state)\n",
    "    \n",
    "def sample_action(state):\n",
    "    global EPS_GREEDY\n",
    "    current_eps = np.maximum(0.01, EPS_GREEDY)\n",
    "    EPS_GREEDY -= EPS_DEC\n",
    "    if np.random.rand() < (1.0 - current_eps): #choose a by policy-NN\n",
    "        action = greedy_action(state) #greedy\n",
    "    else:\n",
    "        action = np.random.randint(n_action) #random action\n",
    "\n",
    "    return torch.tensor([action], dtype=torch.int64) #return tensor\n",
    "\n",
    "def explore_one_step(state, pool):\n",
    "    action = sample_action(state) # a\n",
    "    obs, r, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor(r, dtype=torch.float) # r\n",
    "    next_state = torch.tensor(obs, dtype=torch.float) # s'\n",
    "    terminal = torch.tensor(int(done) * 1.0, dtype=torch.float) # t\n",
    "\n",
    "    # Store the transition in experience pool\n",
    "    pool.push(state, action, reward, next_state, terminal) #(s,a,r,s',t), tensors\n",
    "\n",
    "    return done, next_state, r"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float)\n",
    "    env.render()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float)\n",
    "        if done:\n",
    "            break # one episode\n",
    "\n",
    "def train_loop():\n",
    "    update_policy_steps = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        explore_steps = 0\n",
    "        reward = 0\n",
    "        # Initialize the environment and state\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float) # s\n",
    "        while explore_steps < EPOCH_STEPS:\n",
    "            explore_steps += 1\n",
    "            # generate experience\n",
    "            done, next_state, r = explore_one_step(state, experience_pool)\n",
    "            state = next_state\n",
    "            reward += r\n",
    "            # Perform one step of the optimization\n",
    "            if len(experience_pool) > WARM_UP_SIZE:\n",
    "                update_policy_net()\n",
    "                update_policy_steps += 1\n",
    "                # Update the target network, copying all weights and biases from policy network\n",
    "                if update_policy_steps % TARGET_UPDATE == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "            if done:\n",
    "                break # one episode\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch: \", epoch, \"reward: \", reward)\n",
    "            evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop()"
   ]
  },
  {
   "source": [
    "## 8. Load Saved Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy_net.state_dict(), 'policy-1.pt')\n",
    "policy_net.load_state_dict(torch.load('policy-1.pt'))\n",
    "\n",
    "evaluate()"
   ]
  }
 ]
}