{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Policy Iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial state:  [ 0.04188057  0.0363354  -0.0351086   0.01860545]\nsample action:  1\n#state:  4\n#action:  2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque \n",
    "from itertools import count\n",
    "from itertools import accumulate\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "state = env.reset()\n",
    "print(\"initial state: \", state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(\"sample action: \", action)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(\"#state: \", n_state)\n",
    "print(\"#action: \", n_action)"
   ]
  },
  {
   "source": [
    "## 2. Policy-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = nn.Sequential(\n",
    "        nn.Linear(n_state, 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, n_action),\n",
    "        nn.Softmax())"
   ]
  },
  {
   "source": [
    "## 3. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000\n",
    "STEPS_EPISODES = 200\n",
    "GAMMA = 0.99 #reward-discount: 0.99 is better than 0.90 !!!!!!\n",
    "\n",
    "#optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=0.001)"
   ]
  },
  {
   "source": [
    "## 4. asf "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "def sample_action(state):\n",
    "    with torch.no_grad():\n",
    "        action_prob = policy_net(state).detach().numpy()\n",
    "    action = np.random.choice(np.arange(n_action), p=action_prob)\n",
    "    return action\n",
    "\n",
    "def running_rewards(rewards, gamma): #functional programming\n",
    "    reversed_rewards = list(reversed(rewards))\n",
    "    rewards_r = accumulate(reversed_rewards, lambda c1, c2: c1 * gamma + c2)\n",
    "    return list(reversed(list(rewards_r)))\n",
    "\n",
    "def loss(state):\n",
    "    action_prob = policy_net(state)\n",
    "    action = sample_action(state)\n",
    "    action_onehot = torch.scatter(torch.zeros(n_action), 0, torch.tensor([action]), 1.0) #one-hot action\n",
    "    return torch.dot(torch.log(action_prob), action_onehot)\n",
    "\n",
    "def run_episode():\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    state = torch.tensor(env.reset()) # s\n",
    "    episode_steps = 0\n",
    "    while episode_steps < EPISODE_STEPS:\n",
    "        action = sample_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "                break # one episode\n",
    "\n",
    "\n",
    "\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 5. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # mean square loss\n",
    "    loss = loss_fn(policy_q_value, target_q_value)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float)\n",
    "    env.render()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float)\n",
    "        if done:\n",
    "            break # one episode\n",
    "\n",
    "def train_loop():\n",
    "    update_policy_steps = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        explore_steps = 0\n",
    "        reward = 0\n",
    "        # Initialize the environment and state\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float) # s\n",
    "        while explore_steps < EPOCH_STEPS:\n",
    "            explore_steps += 1\n",
    "            # generate experience\n",
    "            done, next_state, r = explore_one_step(state, experience_pool)\n",
    "            state = next_state\n",
    "            reward += r\n",
    "            # Perform one step of the optimization\n",
    "            if len(experience_pool) > WARM_UP_SIZE:\n",
    "                update_policy_net()\n",
    "                update_policy_steps += 1\n",
    "                # Update the target network, copying all weights and biases from policy network\n",
    "                if update_policy_steps % TARGET_UPDATE == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "            if done:\n",
    "                break # one episode\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch: \", epoch, \"reward: \", reward)\n",
    "            evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop()"
   ]
  },
  {
   "source": [
    "## 8. Load Saved Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy_net.state_dict(), 'policy-1.pt')\n",
    "policy_net.load_state_dict(torch.load('policy-1.pt'))\n",
    "\n",
    "evaluate()"
   ]
  }
 ]
}