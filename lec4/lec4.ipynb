{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Policy Iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial state:  [-0.02427268 -0.04352761  0.04125942 -0.00154008]\nsample action:  1\n#state:  4\n#action:  2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque \n",
    "from itertools import count\n",
    "from itertools import accumulate\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "state = env.reset()\n",
    "print(\"initial state: \", state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(\"sample action: \", action)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(\"#state: \", n_state)\n",
    "print(\"#action: \", n_action)"
   ]
  },
  {
   "source": [
    "## 2. Policy-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = nn.Sequential(\n",
    "        nn.Linear(n_state, 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, n_action),\n",
    "        nn.Softmax())"
   ]
  },
  {
   "source": [
    "## 3. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 20000\n",
    "STEPS_EPISODE = 1000\n",
    "GAMMA = 1\n",
    "\n",
    "#optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=0.001)"
   ]
  },
  {
   "source": [
    "## 4. Explore a Path"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "actions = []\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(state):\n",
    "    with torch.no_grad():\n",
    "        action_prob = policy_net(state).numpy()\n",
    "    action = np.random.choice(np.arange(n_action), p=action_prob)\n",
    "    return action\n",
    "\n",
    "def running_rewards(rewards, gamma): #functional programming\n",
    "    reversed_rewards = list(reversed(rewards))\n",
    "    rewards_r = accumulate(reversed_rewards, lambda c1, c2: c1 * gamma + c2)\n",
    "    return list(reversed(list(rewards_r)))\n",
    "\n",
    "def run_episode():\n",
    "    # clear path\n",
    "    states.clear()\n",
    "    actions.clear()\n",
    "    rewards.clear()\n",
    "\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float)\n",
    "    episode_steps = 0\n",
    "    while episode_steps < STEPS_EPISODE:\n",
    "        action = sample_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        states.append(state)\n",
    "        actions.append(torch.tensor([action]))\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = torch.tensor(next_state, dtype=torch.float)\n",
    "        episode_steps += 1\n",
    "        \n",
    "        if done:\n",
    "                break # one episode"
   ]
  },
  {
   "source": [
    "## 5. Policy Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(states, actions, rewards):\n",
    "    batch_states = torch.stack(states)\n",
    "    actions_prob = policy_net(batch_states)\n",
    "    batch_actions = torch.stack(actions)\n",
    "    actions_onehot = torch.scatter(torch.zeros(len(actions), n_action), 1, batch_actions, 1.0) #one-hot action\n",
    "    actions_cross_entropy = -1.0 * torch.sum(torch.log(actions_prob) * actions_onehot, dim=1)\n",
    "    actions_weight = torch.tensor(running_rewards(rewards, GAMMA))\n",
    "    return torch.mean(actions_cross_entropy * actions_weight)"
   ]
  },
  {
   "source": [
    "## 6. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float)\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float)\n",
    "        if done:\n",
    "            break # one episode\n",
    "\n",
    "def train_loop():\n",
    "    num_episode = 0\n",
    "    while num_episode < EPISODES:\n",
    "        run_episode()\n",
    "        episode_loss = loss(states, actions, rewards)\n",
    "        optimizer.zero_grad()\n",
    "        episode_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if num_episode % 10 == 0:\n",
    "            print(\"episode: \", num_episode, \"reward: \", len(rewards))\n",
    "        if num_episode % 100 == 0:\n",
    "            evaluate()\n",
    "\n",
    "        num_episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loop()"
   ]
  },
  {
   "source": [
    "## 7. Load Saved Model and Evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy_net.state_dict(), 'policy.pt')\n",
    "policy_net.load_state_dict(torch.load('policy.pt'))\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}