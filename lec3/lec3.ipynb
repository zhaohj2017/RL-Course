{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Q Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.00530503 -0.00892214  0.01711064  0.02285505]\n1\n4\n2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "state = env.reset()\n",
    "print(state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(n_state)\n",
    "print(n_action)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion() # interactive mode on: 交互模式"
   ]
  },
  {
   "source": [
    "## 2. Replay Memory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'terminal'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = -1\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None) # allocate space\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.memory[self.position] = Experience(*args) ## append a new experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): ## len(experience)\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "experience_pool = ReplayMemory(10000) #initialize memory pool"
   ]
  },
  {
   "source": [
    "## 3. Q-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "class NN():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(input_size, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, output_size))\n",
    "\n",
    "policy_net = NN(n_state,n_action).model #initialize nn models\n",
    "target_net = NN(n_state,n_action).model\n",
    "target_net.load_state_dict(policy_net.state_dict()) ## copy policy to target"
   ]
  },
  {
   "source": [
    "## 4. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 #batch-train\n",
    "GAMMA = 0.9 #reward-discount\n",
    "EPS = 0.1 #epsilon-greedy\n",
    "TARGET_UPDATE = 10 #policy to target\n",
    "\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "source": [
    "## 5. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_net():\n",
    "    if len(experience_pool) < BATCH_SIZE:\n",
    "        return #not enough experience\n",
    "\n",
    "    experiences = experience_pool.sample(BATCH_SIZE)\n",
    "    experiences_batch = Experience(*zip(*experiences)) #experiences of batches\n",
    "\n",
    "    state_batch = torch.stack(experiences_batch.state)\n",
    "    action_batch = torch.stack(experiences_batch.action)\n",
    "    reward_batch = torch.stack(experiences_batch.reward)\n",
    "    next_state_batch = torch.stack(experiences_batch.next_state)\n",
    "    terminal_batch = torch.stack(experiences_batch.terminal)\n",
    "\n",
    "    output_policy = policy_net(state_batch)\n",
    "    policy_q_value = torch.gather(output_policy, 1, action_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_target_next = target_net(next_state_batch)\n",
    "        target_next_q_value = torch.max(output_target_next, dim=1).values\n",
    "\n",
    "    target_q_value = reward_batch + GAMMA * target_next_q_value * (1 - terminal_batch)\n",
    "\n",
    "    # mean square loss\n",
    "    loss = loss_fn(policy_q_value, target_q_value )\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "## 6. Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def greedy_action(state): # state is tensor\n",
    "    with torch.no_grad():\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "    return action #return integer\n",
    "\n",
    "def sample_action(state):\n",
    "    if np.random.uniform(0, 1) < (1.0 - EPS): #choose a by policy-NN\n",
    "        action = greedy_action(state) #greedy\n",
    "    else:\n",
    "        action = env.action_space.sample() #random action\n",
    "\n",
    "    return torch.tensor([action], dtype=torch.int64) #return tensor\n",
    "\n",
    "def explore_one_step(state, pool):\n",
    "    action = sample_action(state) # a\n",
    "    obs, r, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor(r, dtype=torch.float) # r\n",
    "    next_state = torch.tensor(obs, dtype=torch.float) # s'\n",
    "    terminal = torch.tensor(int(done) * 1.0, dtype=torch.int64) # t\n",
    "\n",
    "    # Store the transition in experience pool\n",
    "    pool.push(state, action, reward, next_state, terminal) #(s,a,r,s',t), tensors\n",
    "\n",
    "    return done, next_state, r"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  0 reward:  14.0\n",
      "epoch:  1 reward:  11.0\n",
      "epoch:  2 reward:  16.0\n",
      "epoch:  3 reward:  25.0\n",
      "epoch:  4 reward:  12.0\n",
      "epoch:  5 reward:  9.0\n",
      "epoch:  6 reward:  9.0\n",
      "epoch:  7 reward:  31.0\n",
      "epoch:  8 reward:  23.0\n",
      "epoch:  9 reward:  15.0\n",
      "epoch:  10 reward:  31.0\n",
      "epoch:  11 reward:  31.0\n",
      "epoch:  12 reward:  22.0\n",
      "epoch:  13 reward:  18.0\n",
      "epoch:  14 reward:  16.0\n",
      "epoch:  15 reward:  47.0\n",
      "epoch:  16 reward:  11.0\n",
      "epoch:  17 reward:  15.0\n",
      "epoch:  18 reward:  12.0\n",
      "epoch:  19 reward:  12.0\n",
      "epoch:  20 reward:  10.0\n",
      "epoch:  21 reward:  28.0\n",
      "epoch:  22 reward:  29.0\n",
      "epoch:  23 reward:  29.0\n",
      "epoch:  24 reward:  11.0\n",
      "epoch:  25 reward:  16.0\n",
      "epoch:  26 reward:  31.0\n",
      "epoch:  27 reward:  12.0\n",
      "epoch:  28 reward:  19.0\n",
      "epoch:  29 reward:  12.0\n",
      "epoch:  30 reward:  24.0\n",
      "epoch:  31 reward:  16.0\n",
      "epoch:  32 reward:  36.0\n",
      "epoch:  33 reward:  21.0\n",
      "epoch:  34 reward:  12.0\n",
      "epoch:  35 reward:  42.0\n",
      "epoch:  36 reward:  15.0\n",
      "epoch:  37 reward:  15.0\n",
      "epoch:  38 reward:  20.0\n",
      "epoch:  39 reward:  15.0\n",
      "epoch:  40 reward:  11.0\n",
      "epoch:  41 reward:  48.0\n",
      "epoch:  42 reward:  18.0\n",
      "epoch:  43 reward:  14.0\n",
      "epoch:  44 reward:  16.0\n",
      "epoch:  45 reward:  20.0\n",
      "epoch:  46 reward:  18.0\n",
      "epoch:  47 reward:  22.0\n",
      "epoch:  48 reward:  35.0\n",
      "epoch:  49 reward:  40.0\n",
      "epoch:  50 reward:  64.0\n",
      "epoch:  51 reward:  22.0\n",
      "epoch:  52 reward:  29.0\n",
      "epoch:  53 reward:  10.0\n",
      "epoch:  54 reward:  20.0\n",
      "epoch:  55 reward:  13.0\n",
      "epoch:  56 reward:  26.0\n",
      "epoch:  57 reward:  16.0\n",
      "epoch:  58 reward:  22.0\n",
      "epoch:  59 reward:  10.0\n",
      "epoch:  60 reward:  38.0\n",
      "epoch:  61 reward:  13.0\n",
      "epoch:  62 reward:  10.0\n",
      "epoch:  63 reward:  31.0\n",
      "epoch:  64 reward:  31.0\n",
      "epoch:  65 reward:  16.0\n",
      "epoch:  66 reward:  16.0\n",
      "epoch:  67 reward:  23.0\n",
      "epoch:  68 reward:  16.0\n",
      "epoch:  69 reward:  17.0\n",
      "epoch:  70 reward:  23.0\n",
      "epoch:  71 reward:  13.0\n",
      "epoch:  72 reward:  17.0\n",
      "epoch:  73 reward:  44.0\n",
      "epoch:  74 reward:  10.0\n",
      "epoch:  75 reward:  10.0\n",
      "epoch:  76 reward:  32.0\n",
      "epoch:  77 reward:  27.0\n",
      "epoch:  78 reward:  9.0\n",
      "epoch:  79 reward:  37.0\n",
      "epoch:  80 reward:  123.0\n",
      "epoch:  81 reward:  39.0\n",
      "epoch:  82 reward:  27.0\n",
      "epoch:  83 reward:  11.0\n",
      "epoch:  84 reward:  54.0\n",
      "epoch:  85 reward:  23.0\n",
      "epoch:  86 reward:  13.0\n",
      "epoch:  87 reward:  23.0\n",
      "epoch:  88 reward:  13.0\n",
      "epoch:  89 reward:  13.0\n",
      "epoch:  90 reward:  18.0\n",
      "epoch:  91 reward:  16.0\n",
      "epoch:  92 reward:  13.0\n",
      "epoch:  93 reward:  40.0\n",
      "epoch:  94 reward:  13.0\n",
      "epoch:  95 reward:  25.0\n",
      "epoch:  96 reward:  13.0\n",
      "epoch:  97 reward:  17.0\n",
      "epoch:  98 reward:  28.0\n",
      "epoch:  99 reward:  11.0\n",
      "epoch:  100 reward:  18.0\n",
      "epoch:  101 reward:  22.0\n",
      "epoch:  102 reward:  13.0\n",
      "epoch:  103 reward:  36.0\n",
      "epoch:  104 reward:  12.0\n",
      "epoch:  105 reward:  12.0\n",
      "epoch:  106 reward:  27.0\n",
      "epoch:  107 reward:  15.0\n",
      "epoch:  108 reward:  15.0\n",
      "epoch:  109 reward:  10.0\n",
      "epoch:  110 reward:  13.0\n",
      "epoch:  111 reward:  14.0\n",
      "epoch:  112 reward:  16.0\n",
      "epoch:  113 reward:  26.0\n",
      "epoch:  114 reward:  20.0\n",
      "epoch:  115 reward:  24.0\n",
      "epoch:  116 reward:  16.0\n",
      "epoch:  117 reward:  24.0\n",
      "epoch:  118 reward:  26.0\n",
      "epoch:  119 reward:  10.0\n",
      "epoch:  120 reward:  38.0\n",
      "epoch:  121 reward:  15.0\n",
      "epoch:  122 reward:  15.0\n",
      "epoch:  123 reward:  27.0\n",
      "epoch:  124 reward:  15.0\n",
      "epoch:  125 reward:  12.0\n",
      "epoch:  126 reward:  11.0\n",
      "epoch:  127 reward:  20.0\n",
      "epoch:  128 reward:  14.0\n",
      "epoch:  129 reward:  13.0\n",
      "epoch:  130 reward:  12.0\n",
      "epoch:  131 reward:  46.0\n",
      "epoch:  132 reward:  12.0\n",
      "epoch:  133 reward:  13.0\n",
      "epoch:  134 reward:  9.0\n",
      "epoch:  135 reward:  16.0\n",
      "epoch:  136 reward:  12.0\n",
      "epoch:  137 reward:  15.0\n",
      "epoch:  138 reward:  15.0\n",
      "epoch:  139 reward:  16.0\n",
      "epoch:  140 reward:  27.0\n",
      "epoch:  141 reward:  12.0\n",
      "epoch:  142 reward:  14.0\n",
      "epoch:  143 reward:  36.0\n",
      "epoch:  144 reward:  43.0\n",
      "epoch:  145 reward:  28.0\n",
      "epoch:  146 reward:  24.0\n",
      "epoch:  147 reward:  9.0\n",
      "epoch:  148 reward:  14.0\n",
      "epoch:  149 reward:  13.0\n",
      "epoch:  150 reward:  11.0\n",
      "epoch:  151 reward:  17.0\n",
      "epoch:  152 reward:  14.0\n",
      "epoch:  153 reward:  9.0\n",
      "epoch:  154 reward:  15.0\n",
      "epoch:  155 reward:  17.0\n",
      "epoch:  156 reward:  12.0\n",
      "epoch:  157 reward:  41.0\n",
      "epoch:  158 reward:  16.0\n",
      "epoch:  159 reward:  12.0\n",
      "epoch:  160 reward:  34.0\n",
      "epoch:  161 reward:  16.0\n",
      "epoch:  162 reward:  12.0\n",
      "epoch:  163 reward:  23.0\n",
      "epoch:  164 reward:  30.0\n",
      "epoch:  165 reward:  40.0\n",
      "epoch:  166 reward:  45.0\n",
      "epoch:  167 reward:  17.0\n",
      "epoch:  168 reward:  19.0\n",
      "epoch:  169 reward:  18.0\n",
      "epoch:  170 reward:  16.0\n",
      "epoch:  171 reward:  27.0\n",
      "epoch:  172 reward:  16.0\n",
      "epoch:  173 reward:  16.0\n",
      "epoch:  174 reward:  28.0\n",
      "epoch:  175 reward:  19.0\n",
      "epoch:  176 reward:  27.0\n",
      "epoch:  177 reward:  10.0\n",
      "epoch:  178 reward:  33.0\n",
      "epoch:  179 reward:  15.0\n",
      "epoch:  180 reward:  18.0\n",
      "epoch:  181 reward:  11.0\n",
      "epoch:  182 reward:  24.0\n",
      "epoch:  183 reward:  19.0\n",
      "epoch:  184 reward:  8.0\n",
      "epoch:  185 reward:  14.0\n",
      "epoch:  186 reward:  21.0\n",
      "epoch:  187 reward:  23.0\n",
      "epoch:  188 reward:  19.0\n",
      "epoch:  189 reward:  24.0\n",
      "epoch:  190 reward:  65.0\n",
      "epoch:  191 reward:  22.0\n",
      "epoch:  192 reward:  44.0\n",
      "epoch:  193 reward:  19.0\n",
      "epoch:  194 reward:  32.0\n",
      "epoch:  195 reward:  17.0\n",
      "epoch:  196 reward:  36.0\n",
      "epoch:  197 reward:  39.0\n",
      "epoch:  198 reward:  15.0\n",
      "epoch:  199 reward:  16.0\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    reward = 0\n",
    "    # Initialize the environment and state\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float) # s\n",
    "    while True:\n",
    "        # generate experience\n",
    "        done, next_state, r = explore_one_step(state, experience_pool)\n",
    "        state = next_state\n",
    "        reward += r\n",
    "        # Perform one step of the optimization\n",
    "        update_policy_net()\n",
    "        # one episode\n",
    "        if done:\n",
    "            print(\"epoch: \", epoch, \"reward: \", reward)\n",
    "            break\n",
    "\n",
    "    # Update the target network, copying all weights and biases from policy network\n",
    "    if epoch % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}