{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Q Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.00877802  0.00412311  0.04100772 -0.02416112]\n0\n4\n2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "state = env.reset()\n",
    "print(state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(n_state)\n",
    "print(n_action)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion() # interactive mode on: 交互模式"
   ]
  },
  {
   "source": [
    "## 2. Replay Memory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'terminal'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = -1\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None) # allocate space\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.memory[self.position] = Experience(*args) ## append a new experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): ## len(experience)\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "experience_pool = ReplayMemory(10000) #initialize memory pool"
   ]
  },
  {
   "source": [
    "## 3. Q-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "class NN():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(input_size, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, output_size))\n",
    "\n",
    "policy_net = NN(n_state,n_action).model #initialize nn models\n",
    "target_net = NN(n_state,n_action).model\n",
    "target_net.load_state_dict(policy_net.state_dict()) ## copy policy to target"
   ]
  },
  {
   "source": [
    "## 4. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 #batch-train\n",
    "GAMMA = 0.9 #reward-discount\n",
    "EPS = 0.1 #epsilon-greedy\n",
    "TARGET_UPDATE = 20 #policy to target\n",
    "\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "source": [
    "## 5. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(experience_pool) < BATCH_SIZE:\n",
    "        return #not enough experience\n",
    "\n",
    "    experiences = experience_pool.sample(2)\n",
    "    experiences_batch = Experience(*zip(*experiences)) #experiences of batches\n",
    "\n",
    "    state_batch = torch.stack(experiences_batch.state)\n",
    "    action_batch = torch.stack(experiences_batch.action)\n",
    "    reward_batch = torch.stack(experiences_batch.reward)\n",
    "    next_state_batch = torch.stack(experiences_batch.next_state)\n",
    "    terminal_batch = torch.stack(experiences_batch.terminal)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "## 6. Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def greedy_action(state): # state is tensor\n",
    "    with torch.no_grad():\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "    return action #return integer\n",
    "\n",
    "def sample_action(state):\n",
    "    if np.random.uniform(0, 1) < (1.0 - EPS): #choose a by policy-NN\n",
    "        action = greedy_action(state) #greedy\n",
    "    else:\n",
    "        action = env.action_space.sample() #random action\n",
    "\n",
    "    return torch.tensor(action, dtype=torch.int) #return tensor\n",
    "\n",
    "def gen_experience(state, pool):\n",
    "    action = sample_action(state) # a\n",
    "    obs, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor(reward) # r\n",
    "    next_state = torch.tensor(obs) # s'\n",
    "    terminal = torch.tensor(int(done), dtype=torch.int) # t\n",
    "\n",
    "    # Store the transition in experience pool\n",
    "    pool.push(state, action, reward, next_state, terminal) #(s,a,r,s',t), tensors\n",
    "\n",
    "    return done, next_state\n",
    "\n",
    "\n",
    "experience_pool.push(torch.tensor([1.0,2.0,3.0,4.0]), torch.tensor(1), torch.tensor(1.0), torch.tensor([5.0,6.0,7.0,8.0]), torch.tensor(0))\n",
    "experience_pool.push(torch.tensor([5.0,6.0,7.0,8.0]), torch.tensor(1), torch.tensor(1.0), torch.tensor([1.0,2.0,3.0,4.0]), torch.tensor(0))\n",
    "print(len(experience_pool))\n",
    "\n",
    "\n",
    "\n",
    "print(action_batch)\n",
    "print(reward_batch)\n",
    "print(terminal_batch)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 93,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\ntensor([1, 1])\ntensor([1., 1.])\ntensor([0, 0])\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset() # s\n",
    "\n",
    "    while True:\n",
    "        # generate experience\n",
    "        done, next_state = gen_experience(state, experience_pool)\n",
    "        state = next_state\n",
    "        # Perform one step of the optimization\n",
    "        optimize_model()\n",
    "        # one episode\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  }
 ]
}