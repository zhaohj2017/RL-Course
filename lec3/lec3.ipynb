{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Q Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.00530503 -0.00892214  0.01711064  0.02285505]\n1\n4\n2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "state = env.reset()\n",
    "print(state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(n_state)\n",
    "print(n_action)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion() # interactive mode on: 交互模式"
   ]
  },
  {
   "source": [
    "## 2. Replay Memory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'terminal'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = -1\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None) # allocate space\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.memory[self.position] = Experience(*args) ## append a new experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): ## len(experience)\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "experience_pool = ReplayMemory(10000) #initialize memory pool"
   ]
  },
  {
   "source": [
    "## 3. Q-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "class NN():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(input_size, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, output_size))\n",
    "\n",
    "policy_net = NN(n_state,n_action).model #initialize nn models\n",
    "target_net = NN(n_state,n_action).model\n",
    "target_net.load_state_dict(policy_net.state_dict()) ## copy policy to target"
   ]
  },
  {
   "source": [
    "## 4. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 #batch-train\n",
    "GAMMA = 0.9 #reward-discount\n",
    "EPS = 0.1 #epsilon-greedy\n",
    "TARGET_UPDATE = 10 #policy to target\n",
    "\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "source": [
    "## 5. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_net():\n",
    "    if len(experience_pool) < BATCH_SIZE:\n",
    "        return #not enough experience\n",
    "\n",
    "    experiences = experience_pool.sample(BATCH_SIZE)\n",
    "    experiences_batch = Experience(*zip(*experiences)) #experiences of batches\n",
    "\n",
    "    state_batch = torch.stack(experiences_batch.state)\n",
    "    action_batch = torch.stack(experiences_batch.action)\n",
    "    reward_batch = torch.stack(experiences_batch.reward)\n",
    "    next_state_batch = torch.stack(experiences_batch.next_state)\n",
    "    terminal_batch = torch.stack(experiences_batch.terminal)\n",
    "\n",
    "    output_policy = policy_net(state_batch)\n",
    "    policy_q_value = torch.gather(output_policy, 1, action_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_target_next = target_net(next_state_batch)\n",
    "        target_next_q_value = torch.max(output_target_next, dim=1).values\n",
    "\n",
    "    target_q_value = reward_batch + GAMMA * target_next_q_value * (1 - terminal_batch)\n",
    "\n",
    "    # mean square loss\n",
    "    loss = loss_fn(policy_q_value, target_q_value )\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "## 6. Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def greedy_action(state): # state is tensor\n",
    "    with torch.no_grad():\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "    return action #return integer\n",
    "\n",
    "def sample_action(state):\n",
    "    if np.random.uniform(0, 1) < (1.0 - EPS): #choose a by policy-NN\n",
    "        action = greedy_action(state) #greedy\n",
    "    else:\n",
    "        action = env.action_space.sample() #random action\n",
    "\n",
    "    return torch.tensor([action], dtype=torch.int64) #return tensor\n",
    "\n",
    "def explore_one_step(state, pool):\n",
    "    action = sample_action(state) # a\n",
    "    obs, r, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor(r, dtype=torch.float) # r\n",
    "    next_state = torch.tensor(obs, dtype=torch.float) # s'\n",
    "    terminal = torch.tensor(int(done) * 1.0, dtype=torch.int64) # t\n",
    "\n",
    "    # Store the transition in experience pool\n",
    "    pool.push(state, action, reward, next_state, terminal) #(s,a,r,s',t), tensors\n",
    "\n",
    "    return done, next_state, r"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  0 reward:  39.0\n",
      "epoch:  1 reward:  10.0\n",
      "epoch:  2 reward:  22.0\n",
      "epoch:  3 reward:  10.0\n",
      "epoch:  4 reward:  14.0\n",
      "epoch:  5 reward:  18.0\n",
      "epoch:  6 reward:  24.0\n",
      "epoch:  7 reward:  34.0\n",
      "epoch:  8 reward:  16.0\n",
      "epoch:  9 reward:  15.0\n",
      "epoch:  10 reward:  12.0\n",
      "epoch:  11 reward:  31.0\n",
      "epoch:  12 reward:  38.0\n",
      "epoch:  13 reward:  12.0\n",
      "epoch:  14 reward:  74.0\n",
      "epoch:  15 reward:  24.0\n",
      "epoch:  16 reward:  35.0\n",
      "epoch:  17 reward:  44.0\n",
      "epoch:  18 reward:  38.0\n",
      "epoch:  19 reward:  15.0\n",
      "epoch:  20 reward:  22.0\n",
      "epoch:  21 reward:  12.0\n",
      "epoch:  22 reward:  17.0\n",
      "epoch:  23 reward:  14.0\n",
      "epoch:  24 reward:  9.0\n",
      "epoch:  25 reward:  16.0\n",
      "epoch:  26 reward:  24.0\n",
      "epoch:  27 reward:  20.0\n",
      "epoch:  28 reward:  22.0\n",
      "epoch:  29 reward:  16.0\n",
      "epoch:  30 reward:  33.0\n",
      "epoch:  31 reward:  15.0\n",
      "epoch:  32 reward:  12.0\n",
      "epoch:  33 reward:  22.0\n",
      "epoch:  34 reward:  16.0\n",
      "epoch:  35 reward:  16.0\n",
      "epoch:  36 reward:  32.0\n",
      "epoch:  37 reward:  18.0\n",
      "epoch:  38 reward:  12.0\n",
      "epoch:  39 reward:  9.0\n",
      "epoch:  40 reward:  15.0\n",
      "epoch:  41 reward:  12.0\n",
      "epoch:  42 reward:  18.0\n",
      "epoch:  43 reward:  48.0\n",
      "epoch:  44 reward:  19.0\n",
      "epoch:  45 reward:  15.0\n",
      "epoch:  46 reward:  14.0\n",
      "epoch:  47 reward:  11.0\n",
      "epoch:  48 reward:  15.0\n",
      "epoch:  49 reward:  11.0\n",
      "epoch:  50 reward:  27.0\n",
      "epoch:  51 reward:  15.0\n",
      "epoch:  52 reward:  54.0\n",
      "epoch:  53 reward:  15.0\n",
      "epoch:  54 reward:  15.0\n",
      "epoch:  55 reward:  20.0\n",
      "epoch:  56 reward:  22.0\n",
      "epoch:  57 reward:  29.0\n",
      "epoch:  58 reward:  10.0\n",
      "epoch:  59 reward:  18.0\n",
      "epoch:  60 reward:  21.0\n",
      "epoch:  61 reward:  14.0\n",
      "epoch:  62 reward:  11.0\n",
      "epoch:  63 reward:  19.0\n",
      "epoch:  64 reward:  11.0\n",
      "epoch:  65 reward:  13.0\n",
      "epoch:  66 reward:  15.0\n",
      "epoch:  67 reward:  15.0\n",
      "epoch:  68 reward:  14.0\n",
      "epoch:  69 reward:  39.0\n",
      "epoch:  70 reward:  18.0\n",
      "epoch:  71 reward:  21.0\n",
      "epoch:  72 reward:  16.0\n",
      "epoch:  73 reward:  14.0\n",
      "epoch:  74 reward:  22.0\n",
      "epoch:  75 reward:  16.0\n",
      "epoch:  76 reward:  17.0\n",
      "epoch:  77 reward:  25.0\n",
      "epoch:  78 reward:  20.0\n",
      "epoch:  79 reward:  13.0\n",
      "epoch:  80 reward:  35.0\n",
      "epoch:  81 reward:  17.0\n",
      "epoch:  82 reward:  18.0\n",
      "epoch:  83 reward:  20.0\n",
      "epoch:  84 reward:  10.0\n",
      "epoch:  85 reward:  17.0\n",
      "epoch:  86 reward:  17.0\n",
      "epoch:  87 reward:  23.0\n",
      "epoch:  88 reward:  17.0\n",
      "epoch:  89 reward:  12.0\n",
      "epoch:  90 reward:  40.0\n",
      "epoch:  91 reward:  8.0\n",
      "epoch:  92 reward:  30.0\n",
      "epoch:  93 reward:  14.0\n",
      "epoch:  94 reward:  31.0\n",
      "epoch:  95 reward:  18.0\n",
      "epoch:  96 reward:  38.0\n",
      "epoch:  97 reward:  9.0\n",
      "epoch:  98 reward:  19.0\n",
      "epoch:  99 reward:  48.0\n",
      "epoch:  100 reward:  15.0\n",
      "epoch:  101 reward:  34.0\n",
      "epoch:  102 reward:  15.0\n",
      "epoch:  103 reward:  35.0\n",
      "epoch:  104 reward:  21.0\n",
      "epoch:  105 reward:  18.0\n",
      "epoch:  106 reward:  9.0\n",
      "epoch:  107 reward:  42.0\n",
      "epoch:  108 reward:  15.0\n",
      "epoch:  109 reward:  12.0\n",
      "epoch:  110 reward:  21.0\n",
      "epoch:  111 reward:  9.0\n",
      "epoch:  112 reward:  10.0\n",
      "epoch:  113 reward:  13.0\n",
      "epoch:  114 reward:  38.0\n",
      "epoch:  115 reward:  26.0\n",
      "epoch:  116 reward:  11.0\n",
      "epoch:  117 reward:  25.0\n",
      "epoch:  118 reward:  18.0\n",
      "epoch:  119 reward:  18.0\n",
      "epoch:  120 reward:  27.0\n",
      "epoch:  121 reward:  13.0\n",
      "epoch:  122 reward:  12.0\n",
      "epoch:  123 reward:  11.0\n",
      "epoch:  124 reward:  17.0\n",
      "epoch:  125 reward:  15.0\n",
      "epoch:  126 reward:  11.0\n",
      "epoch:  127 reward:  21.0\n",
      "epoch:  128 reward:  11.0\n",
      "epoch:  129 reward:  16.0\n",
      "epoch:  130 reward:  24.0\n",
      "epoch:  131 reward:  10.0\n",
      "epoch:  132 reward:  20.0\n",
      "epoch:  133 reward:  17.0\n",
      "epoch:  134 reward:  14.0\n",
      "epoch:  135 reward:  23.0\n",
      "epoch:  136 reward:  17.0\n",
      "epoch:  137 reward:  18.0\n",
      "epoch:  138 reward:  12.0\n",
      "epoch:  139 reward:  19.0\n",
      "epoch:  140 reward:  29.0\n",
      "epoch:  141 reward:  14.0\n",
      "epoch:  142 reward:  14.0\n",
      "epoch:  143 reward:  12.0\n",
      "epoch:  144 reward:  21.0\n",
      "epoch:  145 reward:  34.0\n",
      "epoch:  146 reward:  20.0\n",
      "epoch:  147 reward:  14.0\n",
      "epoch:  148 reward:  14.0\n",
      "epoch:  149 reward:  17.0\n",
      "epoch:  150 reward:  31.0\n",
      "epoch:  151 reward:  58.0\n",
      "epoch:  152 reward:  14.0\n",
      "epoch:  153 reward:  49.0\n",
      "epoch:  154 reward:  44.0\n",
      "epoch:  155 reward:  15.0\n",
      "epoch:  156 reward:  20.0\n",
      "epoch:  157 reward:  35.0\n",
      "epoch:  158 reward:  14.0\n",
      "epoch:  159 reward:  13.0\n",
      "epoch:  160 reward:  33.0\n",
      "epoch:  161 reward:  32.0\n",
      "epoch:  162 reward:  17.0\n",
      "epoch:  163 reward:  31.0\n",
      "epoch:  164 reward:  21.0\n",
      "epoch:  165 reward:  9.0\n",
      "epoch:  166 reward:  44.0\n",
      "epoch:  167 reward:  14.0\n",
      "epoch:  168 reward:  18.0\n",
      "epoch:  169 reward:  10.0\n",
      "epoch:  170 reward:  14.0\n",
      "epoch:  171 reward:  43.0\n",
      "epoch:  172 reward:  13.0\n",
      "epoch:  173 reward:  16.0\n",
      "epoch:  174 reward:  25.0\n",
      "epoch:  175 reward:  13.0\n",
      "epoch:  176 reward:  35.0\n",
      "epoch:  177 reward:  33.0\n",
      "epoch:  178 reward:  12.0\n",
      "epoch:  179 reward:  54.0\n",
      "epoch:  180 reward:  9.0\n",
      "epoch:  181 reward:  43.0\n",
      "epoch:  182 reward:  21.0\n",
      "epoch:  183 reward:  20.0\n",
      "epoch:  184 reward:  48.0\n",
      "epoch:  185 reward:  26.0\n",
      "epoch:  186 reward:  24.0\n",
      "epoch:  187 reward:  15.0\n",
      "epoch:  188 reward:  24.0\n",
      "epoch:  189 reward:  13.0\n",
      "epoch:  190 reward:  33.0\n",
      "epoch:  191 reward:  11.0\n",
      "epoch:  192 reward:  10.0\n",
      "epoch:  193 reward:  17.0\n",
      "epoch:  194 reward:  19.0\n",
      "epoch:  195 reward:  20.0\n",
      "epoch:  196 reward:  16.0\n",
      "epoch:  197 reward:  12.0\n",
      "epoch:  198 reward:  15.0\n",
      "epoch:  199 reward:  10.0\n",
      "epoch:  200 reward:  12.0\n",
      "epoch:  201 reward:  38.0\n",
      "epoch:  202 reward:  22.0\n",
      "epoch:  203 reward:  12.0\n",
      "epoch:  204 reward:  15.0\n",
      "epoch:  205 reward:  13.0\n",
      "epoch:  206 reward:  25.0\n",
      "epoch:  207 reward:  17.0\n",
      "epoch:  208 reward:  18.0\n",
      "epoch:  209 reward:  13.0\n",
      "epoch:  210 reward:  13.0\n",
      "epoch:  211 reward:  12.0\n",
      "epoch:  212 reward:  12.0\n",
      "epoch:  213 reward:  12.0\n",
      "epoch:  214 reward:  18.0\n",
      "epoch:  215 reward:  12.0\n",
      "epoch:  216 reward:  42.0\n",
      "epoch:  217 reward:  47.0\n",
      "epoch:  218 reward:  39.0\n",
      "epoch:  219 reward:  28.0\n",
      "epoch:  220 reward:  14.0\n",
      "epoch:  221 reward:  32.0\n",
      "epoch:  222 reward:  9.0\n",
      "epoch:  223 reward:  14.0\n",
      "epoch:  224 reward:  12.0\n",
      "epoch:  225 reward:  11.0\n",
      "epoch:  226 reward:  12.0\n",
      "epoch:  227 reward:  28.0\n",
      "epoch:  228 reward:  18.0\n",
      "epoch:  229 reward:  40.0\n",
      "epoch:  230 reward:  12.0\n",
      "epoch:  231 reward:  9.0\n",
      "epoch:  232 reward:  11.0\n",
      "epoch:  233 reward:  35.0\n",
      "epoch:  234 reward:  21.0\n",
      "epoch:  235 reward:  18.0\n",
      "epoch:  236 reward:  28.0\n",
      "epoch:  237 reward:  28.0\n",
      "epoch:  238 reward:  11.0\n",
      "epoch:  239 reward:  64.0\n",
      "epoch:  240 reward:  60.0\n",
      "epoch:  241 reward:  32.0\n",
      "epoch:  242 reward:  16.0\n",
      "epoch:  243 reward:  16.0\n",
      "epoch:  244 reward:  24.0\n",
      "epoch:  245 reward:  12.0\n",
      "epoch:  246 reward:  34.0\n",
      "epoch:  247 reward:  19.0\n",
      "epoch:  248 reward:  21.0\n",
      "epoch:  249 reward:  14.0\n",
      "epoch:  250 reward:  24.0\n",
      "epoch:  251 reward:  12.0\n",
      "epoch:  252 reward:  8.0\n",
      "epoch:  253 reward:  16.0\n",
      "epoch:  254 reward:  42.0\n",
      "epoch:  255 reward:  44.0\n",
      "epoch:  256 reward:  19.0\n",
      "epoch:  257 reward:  35.0\n",
      "epoch:  258 reward:  13.0\n",
      "epoch:  259 reward:  14.0\n",
      "epoch:  260 reward:  20.0\n",
      "epoch:  261 reward:  26.0\n",
      "epoch:  262 reward:  10.0\n",
      "epoch:  263 reward:  20.0\n",
      "epoch:  264 reward:  22.0\n",
      "epoch:  265 reward:  12.0\n",
      "epoch:  266 reward:  36.0\n",
      "epoch:  267 reward:  32.0\n",
      "epoch:  268 reward:  36.0\n",
      "epoch:  269 reward:  45.0\n",
      "epoch:  270 reward:  15.0\n",
      "epoch:  271 reward:  31.0\n",
      "epoch:  272 reward:  13.0\n",
      "epoch:  273 reward:  19.0\n",
      "epoch:  274 reward:  14.0\n",
      "epoch:  275 reward:  55.0\n",
      "epoch:  276 reward:  19.0\n",
      "epoch:  277 reward:  23.0\n",
      "epoch:  278 reward:  50.0\n",
      "epoch:  279 reward:  15.0\n",
      "epoch:  280 reward:  26.0\n",
      "epoch:  281 reward:  36.0\n",
      "epoch:  282 reward:  12.0\n",
      "epoch:  283 reward:  19.0\n",
      "epoch:  284 reward:  18.0\n",
      "epoch:  285 reward:  35.0\n",
      "epoch:  286 reward:  16.0\n",
      "epoch:  287 reward:  23.0\n",
      "epoch:  288 reward:  16.0\n",
      "epoch:  289 reward:  17.0\n",
      "epoch:  290 reward:  34.0\n",
      "epoch:  291 reward:  13.0\n",
      "epoch:  292 reward:  55.0\n",
      "epoch:  293 reward:  19.0\n",
      "epoch:  294 reward:  16.0\n",
      "epoch:  295 reward:  23.0\n",
      "epoch:  296 reward:  13.0\n",
      "epoch:  297 reward:  18.0\n",
      "epoch:  298 reward:  14.0\n",
      "epoch:  299 reward:  16.0\n",
      "epoch:  300 reward:  27.0\n",
      "epoch:  301 reward:  36.0\n",
      "epoch:  302 reward:  15.0\n",
      "epoch:  303 reward:  26.0\n",
      "epoch:  304 reward:  14.0\n",
      "epoch:  305 reward:  30.0\n",
      "epoch:  306 reward:  14.0\n",
      "epoch:  307 reward:  17.0\n",
      "epoch:  308 reward:  16.0\n",
      "epoch:  309 reward:  12.0\n",
      "epoch:  310 reward:  21.0\n",
      "epoch:  311 reward:  28.0\n",
      "epoch:  312 reward:  14.0\n",
      "epoch:  313 reward:  24.0\n",
      "epoch:  314 reward:  49.0\n",
      "epoch:  315 reward:  15.0\n",
      "epoch:  316 reward:  28.0\n",
      "epoch:  317 reward:  16.0\n",
      "epoch:  318 reward:  11.0\n",
      "epoch:  319 reward:  12.0\n",
      "epoch:  320 reward:  43.0\n",
      "epoch:  321 reward:  24.0\n",
      "epoch:  322 reward:  22.0\n",
      "epoch:  323 reward:  30.0\n",
      "epoch:  324 reward:  16.0\n",
      "epoch:  325 reward:  20.0\n",
      "epoch:  326 reward:  14.0\n",
      "epoch:  327 reward:  20.0\n",
      "epoch:  328 reward:  15.0\n",
      "epoch:  329 reward:  15.0\n",
      "epoch:  330 reward:  16.0\n",
      "epoch:  331 reward:  17.0\n",
      "epoch:  332 reward:  22.0\n",
      "epoch:  333 reward:  14.0\n",
      "epoch:  334 reward:  17.0\n",
      "epoch:  335 reward:  16.0\n",
      "epoch:  336 reward:  19.0\n",
      "epoch:  337 reward:  13.0\n",
      "epoch:  338 reward:  31.0\n",
      "epoch:  339 reward:  29.0\n",
      "epoch:  340 reward:  27.0\n",
      "epoch:  341 reward:  15.0\n",
      "epoch:  342 reward:  16.0\n",
      "epoch:  343 reward:  12.0\n",
      "epoch:  344 reward:  9.0\n",
      "epoch:  345 reward:  14.0\n",
      "epoch:  346 reward:  15.0\n",
      "epoch:  347 reward:  17.0\n",
      "epoch:  348 reward:  22.0\n",
      "epoch:  349 reward:  16.0\n",
      "epoch:  350 reward:  21.0\n",
      "epoch:  351 reward:  10.0\n",
      "epoch:  352 reward:  21.0\n",
      "epoch:  353 reward:  12.0\n",
      "epoch:  354 reward:  14.0\n",
      "epoch:  355 reward:  13.0\n",
      "epoch:  356 reward:  27.0\n",
      "epoch:  357 reward:  17.0\n",
      "epoch:  358 reward:  14.0\n",
      "epoch:  359 reward:  57.0\n",
      "epoch:  360 reward:  11.0\n",
      "epoch:  361 reward:  16.0\n",
      "epoch:  362 reward:  30.0\n",
      "epoch:  363 reward:  26.0\n",
      "epoch:  364 reward:  14.0\n",
      "epoch:  365 reward:  12.0\n",
      "epoch:  366 reward:  42.0\n",
      "epoch:  367 reward:  35.0\n",
      "epoch:  368 reward:  12.0\n",
      "epoch:  369 reward:  35.0\n",
      "epoch:  370 reward:  12.0\n",
      "epoch:  371 reward:  11.0\n",
      "epoch:  372 reward:  16.0\n",
      "epoch:  373 reward:  22.0\n",
      "epoch:  374 reward:  16.0\n",
      "epoch:  375 reward:  16.0\n",
      "epoch:  376 reward:  15.0\n",
      "epoch:  377 reward:  51.0\n",
      "epoch:  378 reward:  22.0\n",
      "epoch:  379 reward:  10.0\n",
      "epoch:  380 reward:  24.0\n",
      "epoch:  381 reward:  15.0\n",
      "epoch:  382 reward:  12.0\n",
      "epoch:  383 reward:  40.0\n",
      "epoch:  384 reward:  11.0\n",
      "epoch:  385 reward:  43.0\n",
      "epoch:  386 reward:  23.0\n",
      "epoch:  387 reward:  24.0\n",
      "epoch:  388 reward:  8.0\n",
      "epoch:  389 reward:  28.0\n",
      "epoch:  390 reward:  49.0\n",
      "epoch:  391 reward:  22.0\n",
      "epoch:  392 reward:  14.0\n",
      "epoch:  393 reward:  25.0\n",
      "epoch:  394 reward:  32.0\n",
      "epoch:  395 reward:  38.0\n",
      "epoch:  396 reward:  42.0\n",
      "epoch:  397 reward:  21.0\n",
      "epoch:  398 reward:  9.0\n",
      "epoch:  399 reward:  17.0\n",
      "epoch:  400 reward:  42.0\n",
      "epoch:  401 reward:  13.0\n",
      "epoch:  402 reward:  15.0\n",
      "epoch:  403 reward:  48.0\n",
      "epoch:  404 reward:  10.0\n",
      "epoch:  405 reward:  28.0\n",
      "epoch:  406 reward:  15.0\n",
      "epoch:  407 reward:  16.0\n",
      "epoch:  408 reward:  14.0\n",
      "epoch:  409 reward:  12.0\n",
      "epoch:  410 reward:  21.0\n",
      "epoch:  411 reward:  14.0\n",
      "epoch:  412 reward:  13.0\n",
      "epoch:  413 reward:  18.0\n",
      "epoch:  414 reward:  19.0\n",
      "epoch:  415 reward:  44.0\n",
      "epoch:  416 reward:  9.0\n",
      "epoch:  417 reward:  20.0\n",
      "epoch:  418 reward:  20.0\n",
      "epoch:  419 reward:  64.0\n",
      "epoch:  420 reward:  32.0\n",
      "epoch:  421 reward:  12.0\n",
      "epoch:  422 reward:  9.0\n",
      "epoch:  423 reward:  12.0\n",
      "epoch:  424 reward:  29.0\n",
      "epoch:  425 reward:  20.0\n",
      "epoch:  426 reward:  13.0\n",
      "epoch:  427 reward:  11.0\n",
      "epoch:  428 reward:  11.0\n",
      "epoch:  429 reward:  15.0\n",
      "epoch:  430 reward:  11.0\n",
      "epoch:  431 reward:  9.0\n",
      "epoch:  432 reward:  9.0\n",
      "epoch:  433 reward:  48.0\n",
      "epoch:  434 reward:  14.0\n",
      "epoch:  435 reward:  27.0\n",
      "epoch:  436 reward:  21.0\n",
      "epoch:  437 reward:  18.0\n",
      "epoch:  438 reward:  19.0\n",
      "epoch:  439 reward:  24.0\n",
      "epoch:  440 reward:  15.0\n",
      "epoch:  441 reward:  25.0\n",
      "epoch:  442 reward:  28.0\n",
      "epoch:  443 reward:  26.0\n",
      "epoch:  444 reward:  18.0\n",
      "epoch:  445 reward:  15.0\n",
      "epoch:  446 reward:  10.0\n",
      "epoch:  447 reward:  10.0\n",
      "epoch:  448 reward:  18.0\n",
      "epoch:  449 reward:  23.0\n",
      "epoch:  450 reward:  21.0\n",
      "epoch:  451 reward:  19.0\n",
      "epoch:  452 reward:  13.0\n",
      "epoch:  453 reward:  16.0\n",
      "epoch:  454 reward:  13.0\n",
      "epoch:  455 reward:  9.0\n",
      "epoch:  456 reward:  15.0\n",
      "epoch:  457 reward:  23.0\n",
      "epoch:  458 reward:  18.0\n",
      "epoch:  459 reward:  33.0\n",
      "epoch:  460 reward:  11.0\n",
      "epoch:  461 reward:  11.0\n",
      "epoch:  462 reward:  17.0\n",
      "epoch:  463 reward:  16.0\n",
      "epoch:  464 reward:  16.0\n",
      "epoch:  465 reward:  10.0\n",
      "epoch:  466 reward:  22.0\n",
      "epoch:  467 reward:  17.0\n",
      "epoch:  468 reward:  17.0\n",
      "epoch:  469 reward:  22.0\n",
      "epoch:  470 reward:  24.0\n",
      "epoch:  471 reward:  17.0\n",
      "epoch:  472 reward:  19.0\n",
      "epoch:  473 reward:  11.0\n",
      "epoch:  474 reward:  32.0\n",
      "epoch:  475 reward:  28.0\n",
      "epoch:  476 reward:  10.0\n",
      "epoch:  477 reward:  12.0\n",
      "epoch:  478 reward:  18.0\n",
      "epoch:  479 reward:  16.0\n",
      "epoch:  480 reward:  20.0\n",
      "epoch:  481 reward:  19.0\n",
      "epoch:  482 reward:  37.0\n",
      "epoch:  483 reward:  15.0\n",
      "epoch:  484 reward:  31.0\n",
      "epoch:  485 reward:  20.0\n",
      "epoch:  486 reward:  35.0\n",
      "epoch:  487 reward:  30.0\n",
      "epoch:  488 reward:  14.0\n",
      "epoch:  489 reward:  28.0\n",
      "epoch:  490 reward:  13.0\n",
      "epoch:  491 reward:  22.0\n",
      "epoch:  492 reward:  25.0\n",
      "epoch:  493 reward:  9.0\n",
      "epoch:  494 reward:  19.0\n",
      "epoch:  495 reward:  31.0\n",
      "epoch:  496 reward:  22.0\n",
      "epoch:  497 reward:  14.0\n",
      "epoch:  498 reward:  18.0\n",
      "epoch:  499 reward:  21.0\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "for epoch in range(EPOCHS):\n",
    "    reward = 0\n",
    "    # Initialize the environment and state\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float) # s\n",
    "    while True:\n",
    "        # generate experience\n",
    "        done, next_state, r = explore_one_step(state, experience_pool)\n",
    "        state = next_state\n",
    "        reward += r\n",
    "        # Perform one step of the optimization\n",
    "        update_policy_net()\n",
    "        # one episode\n",
    "        if done:\n",
    "            print(\"epoch: \", epoch, \"reward: \", reward)\n",
    "            break\n",
    "\n",
    "    # Update the target network, copying all weights and biases from policy network\n",
    "    if epoch % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}