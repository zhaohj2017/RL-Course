{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Q Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Setup environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque \n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "n_action = env.action_space.n\n",
    "n_state = env.observation_space.shape[0]\n",
    "\n",
    "# env.reset()\n",
    "# screen = env.render(mode='rgb_array') #HWC\n",
    "# print(screen.shape)"
   ]
  },
  {
   "source": [
    "### 1.1 Extract Image Input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize((32,120)),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    # 800x1200x3\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    screen_height, screen_width, _ = screen.shape # HWC\n",
    "    screen = screen[int(screen_height*0.4):int(screen_height * 0.8), :] ## 320 * 1200\n",
    "    # Resize\n",
    "    img = resize(screen) #CHW: 1*32*60\n",
    "    return img.unsqueeze(0)\n",
    "\n",
    "env.reset()\n",
    "img = get_screen().squeeze(0).squeeze(0)\n",
    "print(img.size())\n",
    "plt.figure()\n",
    "plt.imshow(img.numpy(), cmap ='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_state(img_state):\n",
    "    if img_state is None:\n",
    "        pre_img = get_screen()\n",
    "        curr_img = pre_img\n",
    "        return torch.cat((pre_img, curr_img), 1) #torch.cat() vs torch.stack()\n",
    "    else:\n",
    "        pre_img = img_state[0:, 1:, :]\n",
    "        curr_img = get_screen()\n",
    "        return torch.cat((pre_img, curr_img), 1)    "
   ]
  },
  {
   "source": [
    "## 2. Replay Memory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'terminal'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Experience(*args)) ## append a new experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): ## len(experience)\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "experience_pool = ReplayMemory(100000) #initialize memory pool"
   ]
  },
  {
   "source": [
    "## 3. Q-Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = nn.Sequential(\n",
    "        nn.Conv2d(2, 6, (5, 13)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(6, 16, (5, 13)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(16 * 5 * 21, 120), # ???\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, 2))\n",
    "\n",
    "target_net = nn.Sequential(\n",
    "        nn.Conv2d(2, 6, (5, 13)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(6, 16, (5, 13)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(16 * 5 * 21, 120), # ???\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, 2))\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict()) ## copy policy to target"
   ]
  },
  {
   "source": [
    "## 4. Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2001\n",
    "EPOCH_STEPS = 200\n",
    "BATCH_SIZE = 64 #batch-train\n",
    "WARM_UP_SIZE = BATCH_SIZE\n",
    "GAMMA = 0.999 #reward-discount: 0.99 is better than 0.90 !!!!!!\n",
    "EPS_GREEDY = 0.9 #epsilon-greedy\n",
    "EPS_END = 0.005\n",
    "EPS_DEC = 4e-5 #adaptive epsilon greedy\n",
    "TARGET_UPDATE = 250 #policy to target\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "#optimizer = optim.SGD(policy_net.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "source": [
    "## 5. Optimize\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_net():\n",
    "    experiences = experience_pool.sample(BATCH_SIZE)\n",
    "    experiences_batch = Experience(*zip(*experiences)) #experiences of batches\n",
    "\n",
    "    state_batch = torch.cat(experiences_batch.state, 0)\n",
    "    action_batch = torch.stack(experiences_batch.action)\n",
    "    reward_batch = torch.stack(experiences_batch.reward)\n",
    "    next_state_batch = torch.cat(experiences_batch.next_state, 0)\n",
    "    terminal_batch = torch.stack(experiences_batch.terminal)\n",
    "\n",
    "    output_policy = policy_net(state_batch)\n",
    "    policy_q_value = torch.squeeze(torch.gather(output_policy, 1, action_batch))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_target_next = target_net(next_state_batch)\n",
    "        target_next_q_value = torch.max(output_target_next, dim=1).values\n",
    "\n",
    "    target_q_value = reward_batch + GAMMA * target_next_q_value * (1 - terminal_batch)\n",
    "\n",
    "    # mean square loss\n",
    "    loss = loss_fn(policy_q_value, target_q_value)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "## 6. Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def greedy_action(state): # state is tensor\n",
    "    with torch.no_grad():\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "    return action #return integer\n",
    "\n",
    "def sample_action(state):\n",
    "    global EPS_GREEDY\n",
    "    current_eps = np.maximum(EPS_END, EPS_GREEDY)\n",
    "    EPS_GREEDY -= EPS_DEC\n",
    "    if np.random.rand() < (1.0 - current_eps): #choose a by policy-NN\n",
    "        action = greedy_action(state) #greedy\n",
    "    else:\n",
    "        action = np.random.randint(n_action) #random action\n",
    "\n",
    "    return torch.tensor([action], dtype=torch.int64) #return tensor\n",
    "\n",
    "def explore_one_step(state, pool):\n",
    "    action = sample_action(state) # a\n",
    "    _, r, done, _ = env.step(action.item())\n",
    "\n",
    "    reward = torch.tensor(r, dtype=torch.float) # r\n",
    "    \n",
    "    next_state = get_img_state(state) # s'\n",
    "    \n",
    "    terminal = torch.tensor(int(done) * 1.0, dtype=torch.float) # t\n",
    "\n",
    "    # Store the transition in experience pool\n",
    "    pool.push(state, action, reward, next_state, terminal) #(s,a,r,s',t), tensors\n",
    "\n",
    "    return done, next_state, r"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## 7. Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    env.reset()\n",
    "    state = get_img_state(None)\n",
    "    while True:\n",
    "        action = torch.argmax(policy_net(state)).item()\n",
    "        _, _, done, _ = env.step(action)\n",
    "        \n",
    "        state = get_img_state(state) # s'\n",
    "\n",
    "        if done:\n",
    "            break # one episode\n",
    "\n",
    "def train_loop():\n",
    "    update_policy_steps = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        explore_steps = 0\n",
    "        reward = 0\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        state = get_img_state(None)\n",
    "        while explore_steps < EPOCH_STEPS:\n",
    "            explore_steps += 1\n",
    "            # generate experience\n",
    "            done, next_state, r = explore_one_step(state, experience_pool)\n",
    "            state = next_state\n",
    "            reward += r\n",
    "            # Perform one step of the optimization\n",
    "            if len(experience_pool) > WARM_UP_SIZE:\n",
    "                update_policy_net()\n",
    "                update_policy_steps += 1\n",
    "                # Update the target network, copying all weights and biases from policy network\n",
    "                if update_policy_steps % TARGET_UPDATE == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "            if done:\n",
    "                break # one episode\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch: \", epoch, \"reward: \", reward, \"eps: \", np.maximum(EPS_END, EPS_GREEDY), \"pool size: \", len(experience_pool))\n",
    "            evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop()"
   ]
  },
  {
   "source": [
    "## 8. Load Saved Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'policy-2.pt')\n",
    "policy_net.load_state_dict(torch.load('policy-2.pt'))\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}